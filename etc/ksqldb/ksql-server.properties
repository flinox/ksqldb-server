#
# Copyright 2018 Confluent Inc.
#
# Licensed under the Confluent Community License (the "License"); you may not use
# this file except in compliance with the License.  You may obtain a copy of the
# License at
#
# http://www.confluent.io/confluent-community-license
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OF ANY KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations under the License.
#

#------ Endpoint config -------

### HTTP  ###
# The URL the KSQL server will listen on:
# The default is any IPv4 interface on the machine.
# NOTE: If set to wildcard or loopback set 'advertised.listener' to enable pull queries across machines
listeners=http://0.0.0.0:8088

# Use the 'listeners' line below for any IPv6 interface on the machine.
# listeners=http://[::]:8088

# If running a multi-node cluster across multiple machines and 'listeners' is set to a wildcard or loopback address
# 'advertised.listener' must be set to the URL other KSQL nodes should use to reach this node.
# advertised.listener=?

### HTTPS ###
# To switch KSQL over to communicating using HTTPS comment out the 'listeners' line above
# uncomment and complete the properties below.
# See: https://docs.ksqldb.io/en/latest/operate-and-deploy/installation/server-config/security/#configure-the-cli-for-https
#
# listeners=https://0.0.0.0:8088
# advertised.listener=?
# ssl.truststore.location=?
# ssl.truststore.password=?
# ssl.keystore.location=?
# ssl.keystore.password=?
# ssl.key.password=?

#------ Logging config -------

# Automatically create the processing log topic if it does not already exist:
ksql.logging.processing.topic.auto.create=true

# Automatically create a stream within KSQL for the processing log:
ksql.logging.processing.stream.auto.create=true

ksql.log4j.processing.log.brokerlist=${file:/properties/access.properties:KC_BROKERS}
ksql.log4j.processing.log.topic=ksql-processing-log-topic
ksql.logging.processing.topic.name=ksql-processing-log-topic
ksql.log4j.logger.Controller = TRACE,error,stdout
ksql.log4j.logger.Client = TRACE,error,stdout
ksql.log4j.root.loglevel=TRACE
ksql.tools.log4j.loglevel=TRACE


# Uncomment the following if you wish the errors written to the processing log to include the
# contents of the row that caused the error.
# Note: care should be taken to restrict access to the processing topic if the data KSQL is
# processing contains sensitive information.
#ksql.logging.processing.rows.include=true

#------- Metrics config --------

# Turn on collection of metrics of function invocations:
# ksql.udf.collect.metrics=true

#------ External service config -------

#------ Kafka -------

# The set of Kafka brokers to bootstrap Kafka cluster information from:
bootstrap.servers=${file:/properties/access.properties:KC_BROKERS}

# Enable snappy compression for the Kafka producers
compression.type=snappy

#------ Connect -------

# uncomment the below to start an embedded Connect worker
# ksql.connect.worker.config=config/connect.properties

#------ Schema Registry -------

# Uncomment and complete the following to enable KSQL's integration to the Confluent Schema Registry:
ksql.schema.registry.url=${file:/properties/access.properties:SR_URL}


# Security
config.providers=file
config.providers.file.class=org.apache.kafka.common.config.provider.FileConfigProvider


# Schema Registry specific settings
# The converters specify the format of data in Kafka and how to translate it into Connect data.
# Every Connect user will need to configure these based on the format they want their data in
# when loaded from or stored into Kafka
key.converter=io.confluent.connect.avro.AvroConverter
key.converter.basic.auth.credentials.source=USER_INFO
key.converter.schema.registry.basic.auth.user.info=${file:/properties/access.properties:SR_API_KEY}:${file:/properties/access.properties:SR_API_SECRET}
key.converter.schema.registry.url=${file:/properties/access.properties:SR_URL}

value.converter=io.confluent.connect.avro.AvroConverter
value.converter.basic.auth.credentials.source=USER_INFO
value.converter.schema.registry.basic.auth.user.info=${file:/properties/access.properties:SR_API_KEY}:${file:/properties/access.properties:SR_API_SECRET}
value.converter.schema.registry.url=${file:/properties/access.properties:SR_URL}

ksql.internal.topic.replicas=3
ksql.logging.processing.topic.replication.factor=3

ksql.streams.replication.factor=3 
ksql.security.protocol=SASL_SSL
ksql.sasl.mechanism=PLAIN
ksql.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="${file:/properties/access.properties:KC_API_KEY}" password="${file:/properties/access.properties:KC_API_SECRET}";
ksql.schema.registry.basic.auth.credentials.source=USER_INFO
ksql.schema.registry.basic.auth.user.info=${file:/properties/access.properties:SR_API_KEY}:${file:/properties/access.properties:SR_API_SECRET}
ksql.schema.registry.url=${file:/properties/access.properties:SR_URL}
ksql.server.command.response.timeout.ms=15000


ssl.endpoint.identification.algorithm=https
security.protocol=SASL_SSL
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="${file:/properties/access.properties:KC_API_KEY}" password="${file:/properties/access.properties:KC_API_SECRET}";
